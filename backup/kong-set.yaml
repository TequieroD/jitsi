---
# Source: kong/templates/service-account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: jitsi-gw-kong
  namespace: jitsi
  labels:
    app.kubernetes.io/name: kong
    helm.sh/chart: kong-2.13.1
    app.kubernetes.io/instance: "jitsi-gw"
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/version: "3.0"
---
# Source: kong/templates/secret-sa-token.yaml
apiVersion: v1
kind: Secret
metadata:
  name: jitsi-gw-kong-token 
  namespace: jitsi
  annotations:
    kubernetes.io/service-account.name: jitsi-gw-kong
type: kubernetes.io/service-account-token
---
# Source: kong/templates/controller-rbac-resources.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/name: kong
    helm.sh/chart: kong-2.13.1
    app.kubernetes.io/instance: "jitsi-gw"
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/version: "3.0"
  name: jitsi-gw-kong
rules:
- apiGroups:
  - ""
  resources:
  - endpoints
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - endpoints/status
  verbs:
  - get
  - patch
  - update
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - create
  - patch
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - secrets
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - secrets/status
  verbs:
  - get
  - patch
  - update
- apiGroups:
  - ""
  resources:
  - services
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - services/status
  verbs:
  - get
  - patch
  - update
- apiGroups:
  - configuration.konghq.com
  resources:
  - ingressclassparameterses
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - configuration.konghq.com
  resources:
  - kongconsumers
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - configuration.konghq.com
  resources:
  - kongconsumers/status
  verbs:
  - get
  - patch
  - update
- apiGroups:
  - configuration.konghq.com
  resources:
  - kongingresses
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - configuration.konghq.com
  resources:
  - kongingresses/status
  verbs:
  - get
  - patch
  - update
- apiGroups:
  - configuration.konghq.com
  resources:
  - kongplugins
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - configuration.konghq.com
  resources:
  - kongplugins/status
  verbs:
  - get
  - patch
  - update
- apiGroups:
  - configuration.konghq.com
  resources:
  - tcpingresses
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - configuration.konghq.com
  resources:
  - tcpingresses/status
  verbs:
  - get
  - patch
  - update
- apiGroups:
  - configuration.konghq.com
  resources:
  - udpingresses
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - configuration.konghq.com
  resources:
  - udpingresses/status
  verbs:
  - get
  - patch
  - update
- apiGroups:
  - extensions
  resources:
  - ingresses
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - extensions
  resources:
  - ingresses/status
  verbs:
  - get
  - patch
  - update
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses/status
  verbs:
  - get
  - patch
  - update
- apiGroups:
  - configuration.konghq.com
  resources:
  - kongclusterplugins
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - configuration.konghq.com
  resources:
  - kongclusterplugins/status
  verbs:
  - get
  - patch
  - update
- apiGroups:
  - networking.k8s.io
  resources:
  - ingressclasses
  verbs:
  - get
  - list
  - watch
---
# Source: kong/templates/controller-rbac-resources.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: jitsi-gw-kong
  labels:
    app.kubernetes.io/name: kong
    helm.sh/chart: kong-2.13.1
    app.kubernetes.io/instance: "jitsi-gw"
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/version: "3.0"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: jitsi-gw-kong
subjects:
  - kind: ServiceAccount
    name: jitsi-gw-kong
    namespace: jitsi
---
# Source: kong/templates/controller-rbac-resources.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: jitsi-gw-kong
  namespace: jitsi
  labels:
    app.kubernetes.io/name: kong
    helm.sh/chart: kong-2.13.1
    app.kubernetes.io/instance: "jitsi-gw"
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/version: "3.0"
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - pods
      - secrets
      - namespaces
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - configmaps
    resourceNames:
      # Defaults to "<election-id>-<ingress-class>"
      # Here: "<kong-ingress-controller-leader-nginx>-<nginx>"
      # This has to be adapted if you change either parameter
      # when launching the nginx-ingress-controller.
      - "kong-ingress-controller-leader-kong-kong"
    verbs:
      - get
      - update
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - create
  - apiGroups:
      - ""
    resources:
      - endpoints
    verbs:
      - get
  # Begin KIC 2.x leader permissions
  - apiGroups:
      - ""
      - coordination.k8s.io
    resources:
      - configmaps
      - leases
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:
      - ""
    resources:
      - services
      - endpoints
    verbs:
      - get
---
# Source: kong/templates/controller-rbac-resources.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: jitsi-gw-kong
  namespace: jitsi
  labels:
    app.kubernetes.io/name: kong
    helm.sh/chart: kong-2.13.1
    app.kubernetes.io/instance: "jitsi-gw"
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/version: "3.0"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: jitsi-gw-kong
subjects:
  - kind: ServiceAccount
    name: jitsi-gw-kong
    namespace: jitsi
---
# Source: kong/templates/service-kong-udp-proxy.yaml
apiVersion: v1
kind: Service
metadata:
  name: jitsi-gw-kong-udp-proxy
  namespace: jitsi
  labels:
    app.kubernetes.io/name: kong
    helm.sh/chart: kong-2.13.1
    app.kubernetes.io/instance: "jitsi-gw"
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/version: "3.0"
spec:
  type: LoadBalancer
  loadBalancerIP: {{ .Values.jvb.service.loadBalancerIP }}
  ports:
  {{- range $shardName, $shardConfig := .Values.global.shards }}
  {{- $fullShardName := include "jitsi-meet.fullShardName" (dict "root" $ "shardName" $shardName) -}}
  {{- with $ }}
  {{- $maxReplicas := .Values.jvb.replicaCount -}}
  {{- if .Values.jvb.autoscaling.enabled }}
  {{- $maxReplicas = .Values.jvb.autoscaling.maxReplicas -}}
  {{- end }}
  {{- range $replica := (untilStep 0 (int $maxReplicas) 1) }}
  {{- with $ }}
  {{- $port := add $shardConfig.jvbBasePort $replica }}
  - name: streamudp-{{ $port }}
    port: {{ $port }}
    targetPort: {{ $port }}
    protocol: UDP
  {{- end }}
  {{- end }}
  {{- end }}
  {{- end }}
  selector:
    app.kubernetes.io/name: kong
    app.kubernetes.io/component: app
    app.kubernetes.io/instance: "jitsi-gw"
---
# Source: kong/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jitsi-gw-kong
  namespace:  jitsi
  labels:
    app.kubernetes.io/name: kong
    helm.sh/chart: kong-2.13.1
    app.kubernetes.io/instance: "jitsi-gw"
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/version: "3.0"
    app.kubernetes.io/component: app
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: kong
      app.kubernetes.io/component: app
      app.kubernetes.io/instance: "jitsi-gw"

  template:
    metadata:
      annotations:
        kuma.io/service-account-token-volume: jitsi-gw-kong-token
        kuma.io/gateway: "enabled"
        traffic.sidecar.istio.io/includeInboundPorts: ""
      labels:
        app.kubernetes.io/name: kong
        helm.sh/chart: kong-2.13.1
        app.kubernetes.io/instance: "jitsi-gw"
        app.kubernetes.io/managed-by: "Helm"
        app.kubernetes.io/version: "3.0"
        app.kubernetes.io/component: app
        app: jitsi-gw-kong
        version: "3.0"
    spec:
      serviceAccountName: jitsi-gw-kong
      automountServiceAccountToken: false
      
      initContainers:
      - name: clear-stale-pid
        image: kong:3.0
        imagePullPolicy: IfNotPresent
        securityContext:
        
          {}
        resources:
          {}
        command:
        - "rm"
        - "-vrf"
        - "$KONG_PREFIX/pids"
        env:
         
        - name: KONG_ADMIN_ACCESS_LOG
          value: "/dev/stdout"
        - name: KONG_ADMIN_ERROR_LOG
          value: "/dev/stderr"
        - name: KONG_ADMIN_GUI_ACCESS_LOG
          value: "/dev/stdout"
        - name: KONG_ADMIN_GUI_ERROR_LOG
          value: "/dev/stderr"
        - name: KONG_ADMIN_LISTEN
          value: "127.0.0.1:8444 http2 ssl"
        - name: KONG_CLUSTER_LISTEN
          value: "off"
        - name: KONG_DATABASE
          value: "off"
        - name: KONG_KIC
          value: "on"
        - name: KONG_LUA_PACKAGE_PATH
          value: "/opt/?.lua;/opt/?/init.lua;;"
        - name: KONG_NGINX_WORKER_PROCESSES
          value: "2"
        - name: KONG_PLUGINS
          value: "bundled"
        - name: KONG_PORTAL_API_ACCESS_LOG
          value: "/dev/stdout"
        - name: KONG_PORTAL_API_ERROR_LOG
          value: "/dev/stderr"
        - name: KONG_PREFIX
          value: "/kong_prefix/"
        - name: KONG_PROXY_ACCESS_LOG
          value: "/dev/stdout"
        - name: KONG_PROXY_ERROR_LOG
          value: "/dev/stderr"
        - name: KONG_PROXY_LISTEN
          value: "0.0.0.0:8000, 0.0.0.0:8443 http2 ssl"
        - name: KONG_PROXY_STREAM_ACCESS_LOG
          value: "/dev/stdout basic"
        - name: KONG_PROXY_STREAM_ERROR_LOG
          value: "/dev/stderr"
        - name: KONG_ROUTER_FLAVOR
          value: "traditional"
        - name: KONG_STATUS_ACCESS_LOG
          value: "off"
        - name: KONG_STATUS_ERROR_LOG
          value: "/dev/stderr"
        - name: KONG_STATUS_LISTEN
          value: "0.0.0.0:8100"
        - name: KONG_STREAM_LISTEN
          {{- $export := ""}}
          {{- range $shardName, $shardConfig := .Values.global.shards }}
          {{- with $ }}
          {{- $maxReplicas := .Values.jvb.replicaCount -}}
          {{- if .Values.jvb.autoscaling.enabled }}
          {{- $maxReplicas = .Values.jvb.autoscaling.maxReplicas -}}
          {{- end }}
          {{- range $replica := (untilStep 0 (int $maxReplicas) 1) }}
          {{- with $ }}
          {{- $port := add $shardConfig.jvbBasePort $replica }}
          {{- $export = printf "%s 0.0.0.0:%d udp," $export (int64 $port) -}}
          {{- end }}
          {{- end }}
          {{- end }}
          {{- end }}
          value: "{{ $export | trim  | trimSuffix "," }}"
        volumeMounts:
        - name: jitsi-gw-kong-prefix-dir
          mountPath: /kong_prefix/
        - name: jitsi-gw-kong-tmp
          mountPath: /tmp
      containers:
      - name: ingress-controller
        securityContext:
      
          {}
        args:
        
        ports:
        - name: cmetrics
          containerPort: 10255
          protocol: TCP
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace  
        
        
        - name: CONTROLLER_ELECTION_ID
          value: "kong-ingress-controller-leader-kong"
        - name: CONTROLLER_INGRESS_CLASS
          value: "kong"
        - name: CONTROLLER_KONG_ADMIN_TLS_SKIP_VERIFY
          value: "true"
        - name: CONTROLLER_KONG_ADMIN_URL
          value: "https://localhost:8444"
        - name: CONTROLLER_PUBLISH_SERVICE
          value: "default/jitsi-gw-kong-proxy"
        image: kong/kubernetes-ingress-controller:2.7
        imagePullPolicy: IfNotPresent
      
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        resources:
          {}
        volumeMounts:
        - name: jitsi-gw-kong-token
          mountPath: /var/run/secrets/kubernetes.io/serviceaccount
          readOnly: true
        
      
      - name: "proxy"
        image: kong:3.0
        imagePullPolicy: IfNotPresent
        securityContext:
        
          {}
        env:
         
        - name: KONG_ADMIN_ACCESS_LOG
          value: "/dev/stdout"
        - name: KONG_ADMIN_ERROR_LOG
          value: "/dev/stderr"
        - name: KONG_ADMIN_GUI_ACCESS_LOG
          value: "/dev/stdout"
        - name: KONG_ADMIN_GUI_ERROR_LOG
          value: "/dev/stderr"
        - name: KONG_ADMIN_LISTEN
          value: "127.0.0.1:8444 http2 ssl"
        - name: KONG_CLUSTER_LISTEN
          value: "off"
        - name: KONG_DATABASE
          value: "off"
        - name: KONG_KIC
          value: "on"
        - name: KONG_LUA_PACKAGE_PATH
          value: "/opt/?.lua;/opt/?/init.lua;;"
        - name: KONG_NGINX_WORKER_PROCESSES
          value: "2"
        - name: KONG_PLUGINS
          value: "bundled"
        - name: KONG_PORTAL_API_ACCESS_LOG
          value: "/dev/stdout"
        - name: KONG_PORTAL_API_ERROR_LOG
          value: "/dev/stderr"
        - name: KONG_PREFIX
          value: "/kong_prefix/"
        - name: KONG_PROXY_ACCESS_LOG
          value: "/dev/stdout"
        - name: KONG_PROXY_ERROR_LOG
          value: "/dev/stderr"
        - name: KONG_PROXY_LISTEN
          value: "0.0.0.0:8000, 0.0.0.0:8443 http2 ssl"
        - name: KONG_PROXY_STREAM_ACCESS_LOG
          value: "/dev/stdout basic"
        - name: KONG_PROXY_STREAM_ERROR_LOG
          value: "/dev/stderr"
        - name: KONG_ROUTER_FLAVOR
          value: "traditional"
        - name: KONG_STATUS_ACCESS_LOG
          value: "off"
        - name: KONG_STATUS_ERROR_LOG
          value: "/dev/stderr"
        - name: KONG_STATUS_LISTEN
          value: "0.0.0.0:8100"
        - name: KONG_STREAM_LISTEN
          {{- $export := ""}}
          {{- range $shardName, $shardConfig := .Values.global.shards }}
          {{- with $ }}
          {{- $maxReplicas := .Values.jvb.replicaCount -}}
          {{- if .Values.jvb.autoscaling.enabled }}
          {{- $maxReplicas = .Values.jvb.autoscaling.maxReplicas -}}
          {{- end }}
          {{- range $replica := (untilStep 0 (int $maxReplicas) 1) }}
          {{- with $ }}
          {{- $port := add $shardConfig.jvbBasePort $replica }}
          {{- $export = printf "%s 0.0.0.0:%d udp," $export (int64 $port) -}}
          {{- end }}
          {{- end }}
          {{- end }}
          {{- end }}
          value: "{{ $export | trim | trimSuffix "," }}"
        - name: KONG_NGINX_DAEMON
          value: "off"
        lifecycle:
          preStop:
            exec:
              command:
              - kong
              - quit
              - --wait=15
        ports:
        {{- range $shardName, $shardConfig := .Values.global.shards }}
        {{- $fullShardName := include "jitsi-meet.fullShardName" (dict "root" $ "shardName" $shardName) -}}
        {{- with $ }}
        {{- $maxReplicas := .Values.jvb.replicaCount -}}
        {{- if .Values.jvb.autoscaling.enabled }}
        {{- $maxReplicas = .Values.jvb.autoscaling.maxReplicas -}}
        {{- end }}
        {{- range $replica := (untilStep 0 (int $maxReplicas) 1) }}
        {{- with $ }}
        {{- $port := add $shardConfig.jvbBasePort $replica }}
        - name: streamudp-{{ $port }}
          containerPort: {{ $port }}
          protocol: UDP
        {{- end }}
        {{- end }}
        {{- end }}
        {{- end }}
          containerPort: 8100
          protocol: TCP
        volumeMounts:
          - name: jitsi-gw-kong-prefix-dir
            mountPath: /kong_prefix/
          - name: jitsi-gw-kong-tmp
            mountPath: /tmp
          
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /status
            port: status
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /status
            port: status
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        resources:
          {} 
      securityContext:
        {}
      terminationGracePeriodSeconds: 30
      volumes:
        - name: jitsi-gw-kong-prefix-dir
          emptyDir: 
            sizeLimit: 256Mi
        - name: jitsi-gw-kong-tmp
          emptyDir: 
            sizeLimit: 1Gi
        
        - name: jitsi-gw-kong-token
          secret:
            secretName: jitsi-gw-kong-token
            items:
            - key: token
              path: token
            - key: ca.crt
              path: ca.crt
            - key: namespace
              path: namespace
---
{{- if .Values.kong.enabled -}}
apiVersion: configuration.konghq.com/v1beta1
kind: UDPIngress
metadata:
  name: {{- include "jitsi-meet.fullname" . | nindent 4 }}-jvb-udpingress
  labels:
    {{- include "jitsi-meet.labels" . | nindent 4 }}
  annotations:
    kubernetes.io/ingress.class: kong
spec:
  rules:
{{- range $shardName, $shardConfig := .Values.global.shards }}
{{- $fullShardName := include "jitsi-meet.fullShardName" (dict "root" $ "shardName" $shardName) -}}
{{- with $ }}
{{- $maxReplicas := .Values.jvb.replicaCount -}}
{{- if .Values.jvb.autoscaling.enabled }}
{{- $maxReplicas = .Values.jvb.autoscaling.maxReplicas -}}
{{- end }}
{{- range $replica := (untilStep 0 (int $maxReplicas) 1) }}
{{- with $ }}
{{- $port := add $shardConfig.jvbBasePort $replica }}
  - backend:
      serviceName: "{{ $fullShardName }}-jvb-udp-{{ $replica }}"
      servicePort: {{ $port }}
    port: {{ $port }}
{{- end }}
{{- end }}
{{- end }}
{{- end }}
{{- end }}
